{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "! pip install transformers\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model classes in ðŸ¤— Transformers are designed to be compatible with native\n",
    "PyTorch and TensorFlow 2 and can be used seemlessly with either. In this\n",
    "quickstart, we will show how to fine-tune (or train from scratch) a model\n",
    "using the standard training tools available in either framework. We will also\n",
    "show how to use our included `Trainer` class which\n",
    "handles much of the complexity of training for you.\n",
    "\n",
    "This guide assume that you are already familiar with loading and use our\n",
    "models for inference; otherwise, see the [task summary](https://huggingface.co/transformers/task_summary.html). We also assume\n",
    "that you are familiar with training deep neural networks in either PyTorch or\n",
    "TF2, and focus specifically on the nuances and tools for training models in\n",
    "ðŸ¤— Transformers.\n",
    "\n",
    "Sections:\n",
    "\n",
    "  - [pytorch](#pytorch)\n",
    "  - [tensorflow](#tensorflow)\n",
    "  - [trainer](#trainer)\n",
    "  - [additional-resources](#additional-resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pytorch'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning in native PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model classes in ðŸ¤— Transformers that don't begin with `TF` are\n",
    "[PyTorch Modules](https://pytorch.org/docs/master/generated/torch.nn.Module.html),\n",
    "meaning that you can use them just as you would any model in PyTorch for\n",
    "both inference and optimization.\n",
    "\n",
    "Let's consider the common task of fine-tuning a masked language model like\n",
    "BERT on a sequence classification dataset. When we instantiate a model with\n",
    "`PreTrainedModel.from_pretrained`, the model\n",
    "configuration and pre-trained weights\n",
    "of the specified model are used to initialize the model. The\n",
    "library also includes a number of task-specific final layers or 'heads' whose\n",
    "weights are instantiated randomly when not present in the specified\n",
    "pre-trained model. For example, instantiating a model with\n",
    "`BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)`\n",
    "will create a BERT model instance with encoder weights copied from the\n",
    "`bert-base-uncased` model and a randomly initialized sequence\n",
    "classification head on top of the encoder with an output size of 2. Models\n",
    "are initialized in `eval` mode by default. We can call `model.train()` to\n",
    "put it in train mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is useful because it allows us to make use of the pre-trained BERT\n",
    "encoder and easily train it on whatever sequence classification dataset we\n",
    "choose. We can use any PyTorch optimizer, but our library also provides the\n",
    "`AdamW` optimizer which implements gradient bias\n",
    "correction as well as weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer allows us to apply different hyperpameters for specific\n",
    "parameter groups. For example, we can apply weight decay to all parameters\n",
    "other than bias and layer normalization terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up a simple dummy training batch using\n",
    "`PreTrainedTokenizer.__call__`. This returns a\n",
    "`BatchEncoding` instance which\n",
    "prepares everything we might need to pass to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "text_batch = [\"I love Pixar.\", \"I don't care for Pixar.\"]\n",
    "encoding = tokenizer(text_batch, return_tensors='pt', padding=True, truncation=True)\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call a classification model with the `labels` argument, the first\n",
    "returned element is the Cross Entropy loss between the predictions and the\n",
    "passed labels. Having already set up our optimizer, we can then do a\n",
    "backwards pass and update the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor([1,0]).unsqueeze(0)\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can just get the logits and calculate the loss yourself.\n",
    "The following is equivalent to the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "labels = torch.tensor([1,0]).unsqueeze(0)\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "loss = F.cross_entropy(labels, outputs.logitd)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can train on GPU by calling `to('cuda')` on the model and\n",
    "inputs as usual.\n",
    "\n",
    "We also provide a few learning rate scheduling tools. With the following, we\n",
    "can set up a scheduler which warms up for `num_warmup_steps` and then\n",
    "linearly decays to 0 by the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then all we have to do is call `scheduler.step()` after `optimizer.step()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()\n",
    "scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We highly recommend using `Trainer`, discussed below,\n",
    "which conveniently handles the moving parts of training ðŸ¤— Transformers models\n",
    "with features like mixed precision and easy tensorboard logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, you might be interested in keeping the weights of the\n",
    "pre-trained encoder frozen and optimizing only the weights of the head\n",
    "layers. To do so, simply set the `requires_grad` attribute to `False` on\n",
    "the encoder parameters, which can be accessed with the `base_model`\n",
    "submodule on any task-specific model in the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tensorflow'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning in native TensorFlow 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can also be trained natively in TensorFlow 2. Just as with PyTorch,\n",
    "TensorFlow models can be instantiated with\n",
    "`PreTrainedModel.from_pretrained` to load the weights of\n",
    "the encoder from a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `tensorflow_datasets` to load in the [MRPC dataset](https://www.tensorflow.org/datasets/catalog/glue#gluemrpc) from GLUE. We\n",
    "can then use our built-in\n",
    "`glue_convert_examples_to_features`\n",
    "to tokenize MRPC and convert it to a TensorFlow `Dataset` object. Note that\n",
    "tokenizers are framework-agnostic, so there is no need to prepend `TF` to\n",
    "the pretrained tokenizer name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, glue_convert_examples_to_features\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "data = tfds.load('glue/mrpc')\n",
    "train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')\n",
    "train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can then be compiled and trained as any Keras model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss)\n",
    "model.fit(train_dataset, epochs=2, steps_per_epoch=115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the tight interoperability between TensorFlow and PyTorch models, you\n",
    "can even save the model and then reload it as a PyTorch model (or vice-versa):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "model.save_pretrained('./my_mrpc_model/')\n",
    "pytorch_model = BertForSequenceClassification.from_pretrained('./my_mrpc_model/', from_tf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trainer'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide a simple but feature-complete training and evaluation\n",
    "interface through `Trainer` and\n",
    "`TFTrainer`. You can train, fine-tune,\n",
    "and evaluate any ðŸ¤— Transformers model with a wide range of training options and\n",
    "with built-in features like logging, gradient accumulation, and mixed\n",
    "precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now simply call `trainer.train()` to train and `trainer.evaluate()` to\n",
    "evaluate. You can use your own module as well, but the first\n",
    "argument returned from `forward` must be the loss which you wish to\n",
    "optimize.\n",
    "\n",
    "`Trainer` uses a built-in default function to collate\n",
    "batches and prepare them to be fed into the model. If needed, you can also\n",
    "use the `data_collator` argument to pass your own collator function which\n",
    "takes in the data in the format provided by your dataset and returns a\n",
    "batch ready to be fed into the model. Note that\n",
    "`TFTrainer` expects the passed datasets to be dataset\n",
    "objects from `tensorflow_datasets`.\n",
    "\n",
    "To calculate additional metrics in addition to the loss, you can also define\n",
    "your own `compute_metrics` function and pass it to the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can view the results, including any calculated metrics, by\n",
    "launching tensorboard in your specified `logging_dir` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='additional-resources'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [A lightweight colab demo](https://colab.research.google.com/drive/1-JIJlao4dI-Ilww_NnTc0rxtp-ymgDgM?usp=sharing)\n",
    "  which uses `Trainer` for IMDb sentiment classification.\n",
    "\n",
    "- [ðŸ¤— Transformers Examples](https://github.com/huggingface/transformers/tree/master/examples)\n",
    "  including scripts for training and fine-tuning on GLUE, SQuAD, and several other tasks.\n",
    "\n",
    "- [How to train a language model](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb),\n",
    "  a detailed colab notebook which uses `Trainer` to train a masked language model from scratch on Esperanto.\n",
    "\n",
    "- [ðŸ¤— Transformers Notebooks](https://huggingface.co/transformers/notebooks.html) which contain dozens of example notebooks from the community for\n",
    "  training and using ðŸ¤— Transformers on a variety of tasks."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
