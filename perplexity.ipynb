{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "! pip install transformers\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity of fixed-length models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity (PPL) is one of the most common metrics for evaluating language\n",
    "models. Before diving in, we should note that the metric applies specifically\n",
    "to classical language models (sometimes called autoregressive or causal\n",
    "language models) and is not well defined for masked language models like BERT\n",
    "(see [summary of the models](https://huggingface.co/transformers/model_summary.html)).\n",
    "\n",
    "Perplexity is defined as the exponentiated average log-likelihood of a\n",
    "sequence. If we have a tokenized sequence $X = (x_0, x_1, \\dots, x_t)$,\n",
    "then the perplexity of $X$ is,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{PPL}(X)\n",
    "= \\exp \\left\\{ {-\\frac{1}{t}\\sum_i^t \\log p_\\theta (x_i|x_{<i}) } \\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\log p_\\theta (x_i|x_{<i})$ is the log-likelihood of the ith\n",
    "token conditioned on the preceding tokens $x_{<i}$ according to our\n",
    "model. Intuitively, it can be thought of as an evaluation of the model's\n",
    "ability to predict uniformly among the set of specified tokens in a corpus.\n",
    "Importantly, this means that the tokenization procedure has a direct impact\n",
    "on a model's perplexity which should always be taken into consideration when\n",
    "comparing different models.\n",
    "\n",
    "This is also equivalent to the exponentiation of the cross-entropy between\n",
    "the data and model predictions. For more intuition about perplexity and its\n",
    "relationship to Bits Per Character (BPC) and data compression, check out this\n",
    "[fantastic blog post on The Gradient](https://thegradient.pub/understanding-evaluation-metrics-for-language-models/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating PPL with fixed-length models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we weren't limited by a model's context size, we would evaluate the\n",
    "model's perplexity by autoregressively factorizing a sequence and\n",
    "conditioning on the entire preceding subsequence at each step, as shown\n",
    "below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/ppl_full.gif\" width=\"600\" alt=\"Full decomposition of a sequence with unlimited context length\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with approximate models, however, we typically have a constraint\n",
    "on the number of tokens the model can process. The largest version\n",
    "of [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html), for example, has a fixed length of 1024\n",
    "tokens, so we cannot calculate $p_\\theta(x_t|x_{<t})$ directly when\n",
    "$t$ is greater than 1024.\n",
    "\n",
    "Instead, the sequence is typically broken into subsequences equal to the\n",
    "model's maximum input size. If a model's max input size is $k$, we\n",
    "then approximate the likelihood of a token $x_t$ by conditioning only\n",
    "on the $k-1$ tokens that precede it rather than the entire context.\n",
    "When evaluating the model's perplexity of a sequence, a tempting but\n",
    "suboptimal approach is to break the sequence into disjoint chunks and\n",
    "add up the decomposed log-likelihoods of each segment independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/ppl_chunked.gif\" width=\"600\" alt=\"Suboptimal PPL not taking advantage of full available context\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is quick to compute since the perplexity of each segment can be computed\n",
    "in one forward pass, but serves as a poor approximation of the\n",
    "fully-factorized perplexity and will typically yield a higher (worse) PPL\n",
    "because the model will have less context at most of the prediction steps.\n",
    "\n",
    "Instead, the PPL of fixed-length models should be evaluated with a\n",
    "sliding-window strategy. This involves repeatedly sliding the\n",
    "context window so that the model has more context when making each\n",
    "prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/ppl_sliding.gif\" width=\"600\" alt=\"Sliding window PPL taking advantage of all available context\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a closer approximation to the true decomposition of the\n",
    "sequence probability and will typically yield a more favorable score.\n",
    "The downside is that it requires a separate forward pass for each token in\n",
    "the corpus. A good practical compromise is to employ a strided sliding\n",
    "window, moving the context by larger strides rather than sliding by 1 token a\n",
    "time. This allows computation to procede much faster while still giving the\n",
    "model a large context to make predictions at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Calculating perplexity with GPT-2 in ðŸ¤— Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate this process with GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "device = 'cuda'\n",
    "model_id = 'gpt2-large'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load in the WikiText-2 dataset and evaluate the perplexity using a few\n",
    "different sliding-window strategies. Since this dataset is small and we're\n",
    "just doing one forward pass over the set, we can just load and encode the\n",
    "entire dataset in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp import load_dataset\n",
    "test = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "encodings = tokenizer('\\n\\n'.join(test['text']), return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ðŸ¤— Transformers, we can simply pass the `input_ids` as the `labels`\n",
    "to our model, and the average log-likelihood for each token is returned as\n",
    "the loss. With our sliding window approach, however, there is overlap in the\n",
    "tokens we pass to the model at each iteration. We don't want the\n",
    "log-likelihood for the tokens we're just treating as context to be included\n",
    "in our loss, so we can set these targets to `-100` so that they are\n",
    "ignored. The following is an example of how we could do this with a stride of\n",
    "`512`. This means that the model will have at least 512 tokens for context\n",
    "when calculating the conditional likelihood of any one token (provided there\n",
    "are 512 preceding tokens available to condition on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = model.config.n_positions\n",
    "stride = 512\n",
    "\n",
    "lls = []\n",
    "for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n",
    "    begin_loc = max(i + stride - max_length, 0)\n",
    "    end_loc = i + stride\n",
    "    input_ids = encodings.input_ids[:,begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:,:-stride] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        log_likelihood = outputs[0] * stride\n",
    "\n",
    "    lls.append(log_likelihood)\n",
    "\n",
    "ppl = torch.exp(torch.stack(lls).sum() / i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this with the stride length equal to the max input length is\n",
    "equivalent to the suboptimal, non-sliding-window strategy we discussed above.\n",
    "The smaller the stride, the more context the model will have in making each\n",
    "prediction, and the better the reported perplexity will typically be.\n",
    "\n",
    "When we run the above with `stride = 1024`, i.e. no overlap, the resulting\n",
    "PPL is `19.64`, which is about the same as the `19.93` reported in the\n",
    "GPT-2 paper. By using `stride = 512` and thereby employing our striding\n",
    "window strategy, this jumps down to `16.53`. This is not only a more\n",
    "favorable score, but is calculated in a way that is closer to the true\n",
    "autoregressive decomposition of a sequence likelihood."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
